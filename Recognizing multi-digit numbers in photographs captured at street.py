# -*- coding: utf-8 -*-
"""Great learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17PJALIIFXtmk-vuQUmc39aXGdmag9oxz
"""

import numpy as np
import h5py

#open the file
h5f = h5py.File('/content/drive/My Drive/Colab Notebooks/GreatLearning/SVHN_single_grey1.h5', 'r')

#load the train test and validation set
x_train = h5f['X_train'][:]
y_train = h5f['y_train'][:]
x_test = h5f['X_test'][:]
y_test = h5f['y_test'][:]
x_val = h5f['X_val'][:]
y_val =h5f['y_val'][:]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
print(x_val.shape)
print(y_val.shape)

#importing the library
import keras.datasets as mnist
import keras.utils as np_utils

#fix the random seed reproducibility
seed=7
np.random.seed(seed)

# Commented out IPython magic to ensure Python compatibility.
#visualizing the first 10 images on the dataset
import matplotlib.pyplot as plt
# %matplotlib inline
plt.figure(figsize=(10,1))
for i in range(10):
  plt.subplot(1,10,i+1)
  plt.imshow(x_train[i],cmap='gray')
  plt.axis('off')
plt.show()
print('label of each of the above images',(y_train[0:10]))

x_train = x_train.reshape(x_train.shape[0],1024)
x_test = x_test.reshape(x_test.shape[0],1024)
x_val=x_val.reshape(x_val.shape[0],1024)

#normalize the input 0-255 to 0-1
x_train = x_train/255.0
x_test = x_test/255.0
x_val = x_val/255.0

print('x training shape',x_train.shape)
print('y trian shape',y_train.shape)
print('x test shape',x_test.shape)
print('y test shape',y_test.shape)
print('x val shape',x_val.shape)
print('y val shape',y_val.shape)

#one hot encoder
y_train = np_utils.to_categorical(y_train,num_classes=10)
y_test = np_utils.to_categorical(y_test,num_classes=10)
y_val = np_utils.to_categorical(y_val,num_classes=10)

#convert the x_train,x_test,x_val into flatten
x_tr=[]
for i in range(42000):
  x_tr.append(x_train[i,:].flatten())
x_te=[]
for i in range(18000):
  x_te.append(x_test[i,:].flatten())
x_vl=[]
for i in range(60000):
  x_vl.append(x_val[i,:].flatten())

print(len(x_tr))
print(len(x_te))
print(len(x_vl))
print(len(y_train))
print(len(y_test))
print(len(y_val))

"""###Buid a neural network"""

from keras.layers import BatchNormalization
import keras
from keras import losses
from keras import optimizers
from keras.layers import Dropout, MaxPooling2D
from keras.models import Sequential
from keras.layers import Dense

def nn_model():
  #create a model
  model = Sequential()
  #add batch normalization
  model.add(BatchNormalization(input_shape=(1024,)))

  #add hidden layer
  model.add(Dense(256,activation='relu'))
  model.add(Dense(64,activation='relu'))
  model.add(Dense(64,activation='relu'))
  model.add(Dense(32,activation='relu'))

  #add output layer
  model.add(Dense(10,activation='softmax'))

  #compile the model
  sgd=optimizers.Adam(lr=1e-3)
  model.compile(optimizer=sgd,loss=losses.categorical_crossentropy,metrics=['accuracy'])
  return model

#built the model
model = nn_model()

#fit the model
model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=30,batch_size=200,verbose=2)

#Evaluation of the model
score = model.evaluate(x_test,y_test,verbose=0)

print("Error: %.2f%%" % (100-score[1]*100))

score

model.summary()

